Online survey-based self-assessment tools ::::

FAIR self-assessment Tool: The Australian Research Data Commons (ARDC) (ARDC, 2018) has created the ARDC FAIR self-assessment tool, a questionnaire-based tool designed for informative and instructional purposes (ARDC, 2020). It was influenced by the CSIRO 5-star data rating tool (FORCE11, 2021) and the FAIRdat tool from DANS (Pisani et al., 2021). The web-based ARDC FAIR self-assessment tool consists of a single webpage with all the questions stated. One component corresponds to each of the four FAIR principles, and the questions are a combination of multiple choice and true/false. FAIR Assessment Tool This is an HTML file that allows a user to assess the 'FAIRness' of a dataset. Installation Copy the file index.html from the HTML directory to a server. Authors Kerry Levett, Keith Russell, Martin Schweitzer, Kathryn Unsworth, and Andrew White Contributing Contributions are always welcome! See CONTRIBUTING.md for ways to get started.
 
FAIR data Self-Assessment Tool: This FAIR data Self-Assessment Tool is provided purely for informational purposes. It is based on our interpretation of the FAIR Data Principles with the acknowledgment that there are other interpretations of the principles. The scores arising from this tool are intended for self-assessment purposes only and to trigger thinking and discussion around possible ways of making data more FAIR. Cross-Lateral Enterprises Pty Ltd. Credit: This tool is based on the ARDC FAIR data Self-Assessment Tool developed by ARDC, Although this tool is identical to the ARDC, it produces different results.

FAIR Aware: FAIR-Aware helps you assess your knowledge of the FAIR Principles, and better understand how making your data(set) FAIR can increase the potential value and impact of data. The tool is discipline-agnostic, making it relevant to any scientific field. You can use this tool at any point during your research before depositing data(set) in a data repository. It is also good to remember that many FAIR-related decisions can already be made in the research planning phase, so you may want to use FAIR-Aware early on to help you make those decisions. This tool resembles a four-part manual with four guiding concepts. Every section has a few questions, and if the user is unfamiliar with any of them, there is advice provided to help them comprehend and learn more. There are five knowledge levels, ranging from strongly agree to strongly disagree if the user is aware. and a pop-up providing the question's explanation will show up if the user is unsure. Following the completion of the evaluation, the results will be displayed along with the proportion of "awareness" and "willing to comply." Additionally, it will detail the user's uncertainties during the evaluation on the final page. It will provide the summary also of the assessment if a user wants to see it last. This tool will also assess the FAIR quality of the data and give comprehensive information if the user is not familiar with this concept. Following that, the user can retake the evaluation to see the FAIR %.

FAIRdat: This tool offers a brief examination consisting of a few simple questions for each principle, along with an assessment with findings in the form of an output score. Additionally, it will add points, divide the overall score by 3, and determine the percentile for reusability. Although it will ask for the DOI, this tool won't produce a reliable test result. This instrument works well for experiments. They provide one full document also and this document has the purpose of accompanying the FAIR data assessment tool (survey) designed for re-users of datasets to review data based on our operationalized FAIR principles. Following dataset reviews, each dataset that has been reused will have a published review like you would see on a website for product reviews i.e. amazon. In such product reviews, it is normally the case that star ratings are given by users of the product to rate their experience with, and perceived quality of the item. We want to implement this process in the same way for reused datasets. The goal of this project is to use the FAIR principles as the foundation for a data assessment tool called FAIRdat. This will allow any dataset that is deposited or reused from any data repository to be evaluated for its overall "FAIRness" on a 5-star scale, as well as its score on the principles of Findable, Accessible, Interoperable, and overall. There are currently a few badge programs for open data quality evaluation that allow reviewers to rank datasets according to certain quality metrics.

FAIRDataBR: The tool for assessing FairDataBR datasets emerged from the realization of the need to develop an application that would automate the verification process of the adherence of datasets to the FAIR Principles. This tool was developed by researchers linked to the Federal University of Paraíba (PPGCI/MPGOA - UFPB). It is characterized by being simple and intuitive to use. The FAIRDataBR tool is used to test whether or not the data is FAIR.  The purpose of the tool and the significance of the questions it asks are not explained. It will be simple to use this tool if the user is already familiar with the FAIR idea; but, if the user is not, taking the assessment will be somewhat challenging. Every question includes a reference link to an additional tool; if the user chooses one link among the others, the specifics of the FAIR principles will then be displayed. However, the entire process will take a significant amount of time. In addition to lacking a button to return to the assessment page itself, the user must click back to view the history page to access the assessment page. This is not an acceptable way for any website that aims to guide visitors toward functioning to return to. This means that the user will see five pie bars at the bottom of the page, one for the overall percentage and one for each principle, each with a score out of ten. It all looks nice and makes sense. Additionally, the tool language's Portuguese translation presents one major issue. Although you have to click the translate button each time you go to a new website, there is also an option to translate it into English. Unfortunately, if the user goes back to the main page and clicks on another tool for help, the multiple-choice questions will remain blank, requiring the user to respond once more.

NFDI4Culture FAIR-Check: Questionnaire of 25 questions about FAIRness with connection to NFDI4Culture Helpdesk. The NFDI4Culture FAIR self-check is modified using data from the cultural heritage industry. More detailed explanations and suggestions may be found by following the links to the Guideline for FAIR Cultural Studies Research Data Management (in Portuguese). Because each study project's unique circumstances cannot be represented by percentages or traffic light colors, our check does not appear to be objective. Our questionnaire, on the other hand, aims to highlight areas where data quality may be improved and to promote self-reflection on how one handles research data. The integrated ability to immediately contact the NFDI4Culture Helpdesk with questions and findings is another unique element of the FAIR Check. This allows for face-to-face clarification of specific issues. With a total of 25 questions and in-depth questions covering every concept, this tool is a time-consuming examination. There is no qualitative or quantitative outcome, however, it does provide a brief explanation and some questions. The user-completed assessment's questions and answers will be available in a PDF file. This tool lacks improvement ideas for making data more equitable as well as a grasp of what defines fair data.

SATIFYD: Use the DANS self-assessment tool to increase the dataset's fairness. With the help of this tool, you can assess the FAIRness (Findable, Accessible, Interoperable, Reusable) of your dataset and get advice on how to improve it even further. Using this tool before depositing is ideal.

Online semi-automated tool ::::


FAIRshake: FAIRshake is a toolkit that enables the systematic assessment of the FAIRness of any digital resource. The outcomes of the FAIR assessment are shown as a design that displays the FAIR score as a small grid of red, blue, and purple squares. To conduct FAIR evaluations, FAIRshake offers tools for linking digital items to metrics and rubrics. The FAIR symbol is used to indicate these evaluations. The components of the FAIRshake toolkit consist of a Chrome extension, a bookmarklet, and a full-stack web-server application with a search engine interface, a backend database, and an application programming interface (API). Additionally, FAIRshake has FAIR analytics modules that generate statistical reports on assessments collected for a given project. Documentation for SmartAPI, Swagger/OpenAPI, and CoreAPI is available through the FAIRshake endpoint-REST API, which complies with the FAIR principles. To visualize how successfully a digital resource satisfies the FAIR criteria of the selected rubric, the FAIRshake representation employs a color gradient from blue (good) to red (unsatisfactory). The insignia dynamically extends to fit all evaluations as several rubrics made up of various metrics might be used to evaluate the same digital resources. The squares corresponding with these metrics will be covered in gray if responses to the rubric are absent.
Compared with previous attempts to develop FAIRness assessment tooling, the FAIRshake toolkit has more features. 

    it contains a database that enlists users, projects, digital resources, metrics, rubrics, and assessments. 
    it is a full-stack application with a user interface,
    it comes with a browser extension and a bookmarklet to enable viewing and submitting assessments from any website.


Online automated tools ::::

FAIR enough: To complete the evaluation and provide the findings in just a couple of minutes, FAIR enough is an automated tool that simply requires the URL and DOI. 

    This tool's primary characteristics include community compliance assessments and core universal maturity indicators. 
    Evaluation execution that is stable and quick (the majority of examined resources take less than one minute, and no commercial license is needed).
    A library where new maturity indicators may be defined, published, and registered. 
    To create collections and author assessments, it supports ORCID authentication. A lot of the code for the Metrics tests was taken from "F-UJI," and this effort was mostly influenced by the Ruby "FAIR evaluator." The front end of this tool was created using Material UI and React, while the back end was designed using Pydantic, FastAPI, and MongoDB.
Users may complete the evaluation without logging in, so the first question it asks is, "Provide the URL of the resource or digital object you want to evaluate?" The user will then be transmitted to another website with an option when they click the link. Here, the user has numerous choice options to choose from the following "Select the collection of FAIR Maturity Indicators (aka. FAIR Metrics tests) to validate your resource against:" 1. FAIR enough metadata maturity indicators(fair-enough-metadata) 2. FAIR Evaluator maturity indicators 3. FAIR enough data maturity indicators(fair-enough-data) and 4. FAIR maturity indicators for rare diseases. Users must select one option to begin the evaluation.

F-UJI: The REST API supports a programmatic assessment of the FAIRness of research data objects based on a set of core metrics developed by the FAIRsFAIR project. It is evident from the research paper that each of the 17 metric criteria is specified and described. Therefore, users find it quite straightforward to understand the tool's history and purpose. Using metrics created by the FAIRsFAIR project, F-UJI is a web service that programmatically evaluates the FAIRness of research data items. Utilizing the service will allow for the examination of objects in repositories suitable for direct project cooperation. 'UJI' means 'Test' in Malay, and 'F' stands for FAIR, naturally. FAIR testing tools include F-UJI. The FAIRness of DATA was addressed with a workable solution created by FAIRsFAIR. 'FsF-F1-01D' is the example of one term given to its 17 metrics, where FsF stands for FAIRsFAIR and F1 is the first principle for findable. 01 is the local ID and the final D/M indicates a resource to be examined, such as data or metadata. Except for A1.1, A1.2 (open protocol, authentication, and authorization), and I2 (FAIR vocabulary), the metrics now cover the other FAIR principles. This automated tool test may be completed in a relatively short amount of time, and it produced very good visual results since it displayed all the facts in the manner that each metric stated. So that the first-level user is as well aware of what is missing in their dataset. Every metric constraint and limitation that has to be addressed in the tool's future edition is listed in the document for the tool. Thus, the tool's current degree of development is a really excellent thing. Having the FAIR level with earned scores is one of the output page's advantages. A FAIR level such as "initial," "moderate," and "advanced" will be displayed for each measure based on the maturity level (0–3) and earning score (0–1). This is a really good automated tool that the research field will recommend for usage. The pie chart displaying the percentage of the overall FAIR level will be displayed, in summary. 

FAIR-Checker: The automatic tool FAIR-Checker was created by IFB (ELIXIR-FR) with all digital items as its goal. It is an evaluation web interface with a Python framework designed to make FAIR metrics implementation easier. Its objective is to evaluate the FAIR principles and enable data providers to improve the quality of their digital assets. Both users and data suppliers can assess the FAIRness of online resources. Web resources reveal metadata that developers may examine and analyze. Using established ontologies and restricted vocabularies, FAIR-Checker verifies that metadata adheres to standards by utilizing semantic web technologies.
Initially, web pages are searched for embedded semantic annotations. Next, this minimum knowledge graph is finished using knowledge graphs that have already been deployed, such as Datacite, OpenAire, and WikiData (Step 2). Step 3 involves testing the generated knowledge network using Linked Open Vocabularies (LOV), Ontology Lookup Service (OLS), or Bioportal to ensure that classes and attributes are identified. The resource information is validated against Bioschemas community profiles as the last verification step (Step 4). This project is funded by the French Institute for Bioinformatics (IFB) through the PIA2 11-INBS-0013 grant.

FOOPS!: A web-based application called FOOPS! was created to evaluate how well ontologies and vocabularies adhere to the FAIR principles. To adapt FAIR to semantic artifacts, FOOPS! does 24 distinct tests across the four FAIR dimensions, taking into account current community discussions and best practices. For the FAIR principles, FOOPS! is an ontology pitfall scanner. To give a thorough overview of how a vocabulary conforms with current FAIR best practices for ontologies, FOOPS! works for both OWL and SKOS vocabularies and runs 24 different checks distributed across the FAIR dimensions. It sets itself apart from other services like Vapour7, which is focused on the quality of the content negotiation of resources, and OOPS!, which is focused on common pitfalls of the ontology itself. Findable (9 checks): the service determines if the ontology URI is unique for that version, persistent, resolvable, and has a resolvable version IRI. Accessible (3 checks): FOOPS! will evaluate if the URI protocol is open and whether the ontology has appropriate content negotiation (with at least one RDF serialization and HTML).
Interoperable (3 checks): these checks look for references to pre-existing vocabularies in the data properties, classes, metadata annotations, or properties themselves. Reusable (9 tests): the service checks for the presence of provenance metadata, licensing metadata, and extensive vocabulary metadata, as well as the accuracy of the labels and definitions provided for ontology words.

OpenAIRE Validator - FAIR assessment: The OpenAIRE Validator service allows testing the compatibility of the data sources with the OpenAIRE Guidelines. If validation succeeds the data source can be registered for regular aggregation and indexing in OpenAIRE. OpenAIRE PROVIDE is the content gateway service of OpenAIRE, inviting data providers to connect scholarly content with OpenAIRE. This makes repositories, data archives, journals, aggregators, and CRIS systems accessible to millions of researchers, research institutes and networks, policymakers, and citizens by enabling them to enter the OpenAIRE and European Open Science (EOSC) ecosystem.
OpenAIRE PROVIDE facilitates visual access to OpenAIRE's collection services for its customers by providing many connectors, hence lowering technological hurdles. Before content is available on OpenAIRE and EOSC, there are four main processes involved in the OpenAIRE PROVIDE express of interest process. The stages highlight the crucial OpenAIRE sub-services that carry out the following tasks:
     Validation of data sources with the OpenAIRE guidelines (via the OpenAIRE Validator),
     Registration of data sources to OpenAIRE and global interlinked networks provides links to content for text and data mining, view history of validations, status of harvesting,
     Enrichment of metadata information that describes the data sources to be available through OpenAIRE. Subscribe and view/receive notifications to enrich the metadata or the content of the data source (via the OpenAIRE Broker),
     Generation of usage statistics by subscribing to the OpenAIRE UsageCounts service; Get aggregated, COUNTER-compliant usage statistics, like metadata views and full-text downloads, for repository access and broaden your mechanisms for Open research impact assessment.
	
The menu bar of OpenAIRE contains several choices, including "explore", "provide", "connect", "monitor", and "develop". However, the FAIR test will be conducted in the 'Provide' option. Four choices are available in OpenAIRE ‘provide’ option: the CRIS system, the literature repository, the data repository, and the FAIR assistance. The compatibility test will be conducted in this FAIR Assistance phase. It has four steps, which are as follows: Select Data source ( Select base URL from one of the registered repositories or new one), select guidelines, select parameters, and then Finish. There are certain maturity indicators for each concept in the examination. The results of testing the maturity indicators are displayed in the BAR and RADAR charts.There are seven maturity indicators for findable, eleven for accessible, twelve for interoperable, and nine for reusable. Following this compatibility test, a chart table with the terms "For content" and "For usage" will display the results. It will display the results based on the content, including Rule name, Rule description, Rule weight, number Of records, and status (green tick or red cross). The tool will generate a BAR chart based on usage and content across all records, and a RADAR chart based on maturity indicators.

FAIR EVA (Evaluator, Validator \& Advisor): A novel tool called FAIR EVA has been developed to check the FAIRness level of digital objects from different repositories or data portals. It requires the object identifier (preferably a persistent and unique identifier) and the repository to check. It also provides a generic and agnostic way to check digital objects. It was created within the framework of the European Open Science Cloud. It is focused on specific data management systems, such as open repositories, and may be customized for a particular situation in a scalable and automated setting. It may be set up in a Docker container or as a stand-alone application. It uses two separate web services: an online interface for ease of use and accessibility, and an API for managing the assessment. Considering the flexibility of the FAIR Principles, it attempts to be enough adaptable to function for various contexts, repository software, and disciplines. They started by providing the DIGITAL.CSIC repository, as the tool's initial target, gathers both the institutional repository and the specific requirements of a multidisciplinary university. A web-based tool called FAIR EVA assesses whether or not digital items—primarily research data—are in the DIGITAL.CSIC institutional repository adheres to the FAIR Principles. It is based on the RDA FAIR Data Maturity Indicators and gives institutional repositories' unique qualities particular consideration. User1 was able to use the website [https://fair.csic.es/es] to verify the FAIRness of her data by using the FAIR EVA document. It might become difficult to complete the exam because this website is in Spanish. The website will request to execute the test and ask for the DOI or Handle PID.

FAIR EVA distinguishes itself by providing: Scalability: It is capable of efficiently evaluating a large number of digital objects or datasets. Flexibility and Customization: Its system of plug-ins enables customization for various scientific disciplines, repositories, and data systems.User Guidance and Support: It helps repository administrators and data providers enhance the quality and services provided by their data. Technical Solutions and Integration: It offers semantics and ontologies for smooth integration with current technologies. Relevance to Contemporary Data Contexts: It takes into account the rapidly expanding research product market and the changing Open Science environment, including the European Open Science Cloud (EOSC). The output of this tool will resemble the PIE chart and BAR chart from the study article, complete with percentages displayed in each bar and color filling-in yellow, green, and red. It will also provide a thorough explanation for every RDA, along with the proportion of each and the total FAIR % for the FAIR data. Each RDA indication includes an expanding option that provides these facts indication [indicator level, indicator assessment, technical implementation, technical feedback, tips]. After downloading, it provides a single export PDF option, which provides the PDF file. The total output page indicates that the server's operation is the problem, even when the frontend view is working OK. There are some coding issues, and an internal server error is displayed.

FAIR Evaluator: The FAIR Evaluator is a Ruby on Rails application with four primary components: A (read/write/search) registry of known MI Tests and their annotations, A (read/write/search) registry of collections of MI Tests (“Collections”) and their annotations, An invocation function that creates a pipeline of MI Tests and applies them to a resource, A registry of evaluation results. The Evaluator provides a registry and execution functions for:
Maturity Indicator Tests, Community-defined Collections of Maturity Indicator Tests, and Quantitative FAIRness evaluations of a Resource based on these Collections. Currently, the Evaluator is running on an Nginx server with a Google App Engine (2 CPU, 6 G RAM), and a MySQL 5.7 database on a separate Google Cloud server that handles storage. Three processes with five threads each make up the Ruby on Rails v5.2.3 setup.
This website offers three primary services for FAIR assessment. First, import MI tests (Import Maturity Indicators Tests as YAML smart API interface annotation); next, create collections (Assemble Maturity Indicators Tests into community-centered collections); and last, evaluate resources (Evaluate resources FAIRness against Collections of Maturity Indicator Tests). In order to facilitate users' search for their submissions, the services focus on offering any sort of unique identification. The user authorizes the ORCiD to be associated with this data and to be made publicly available by providing an ORCID. To have their ORCiD removed from this record, the user may get in connect with the maintainers at any moment.

HowFAIRIs: HowFAIRIs is an automated program that uses a Python module as its base to examine if a GitHub or GitLab repository complies with the guidelines provided by fair-software.eu: Fair-Software.EU is the URL. Users must first compile a list of the repositories they wish to test, analyze them, and write a report. Fairtally is different software that users can install to compare repositories and produce a report. It is unable to deal with local files from the local machine. Use the GitHub action that can assist in enabling repositories for that purpose. On the other hand, this tool is made to function with publicly available web repositories. The program supports Markdown and RestructuredText and is compatible with GitHub and GitLab. Fairtally will examine the repositories that display the colorful downloadable badges. The user may modify the parts of the tool they do not want to work with, and the program has output choices with Repository, License, registry, Citation, checklist, count, and badges. Several repositories are having problems. When an issue occurs when working with more than 100 repositories, the user must restart. The tool still has a problem to solve. However, it will take no more than ten minutes to examine all of its results provided there are no errors.
The badge's color is determined by the degree of compliance, and the arrangement of full and empty circles varies according to the guidelines that the repository abides by. The first symbol in the circle denotes the first recommendation, which is to use a publicly available repository with version control. The second symbol indicates the second guideline, and so on. Each circle symbolizes one of the recommendations. A community registry registration for the software is indicated by the third circle's status. The badge becomes red because the repository only follows one of the guidelines. The badge's orange tint indicates that the repository carries out three of the five suggestions. This publicly available repository with version control is based on the open/closed status of the circles. Citation information is included, and it has been recorded in a community registry. Neither a license nor a checklist are used in this repository's project. One receives a yellow badge for nearly perfect compliance. Except for the recommendation requesting the addition of a checklist, the associated repository complies with every guideline. The green indicates perfect compliance.



Offline self-assessment survey-based tools ::::

Do I-PASS for FAIR: While the term FAIR is sometimes used as an adjective for other (digital) topics, such as FAIR data stewardship, FAIR data infrastructure, and FAIR data services, it is intended to be applied to datasets using the 15 FAIR data principles. Furthermore, an increasing number of Dutch research organizations and institutions mention a FAIR organization as a crucial objective in the context of Open Science and scientific integrity, implementing RDM methods and support with the FAIR principles as the primary motivator. A task group from LCRDM (National Coordination Point Research Data Management) was motivated to investigate the definition, traits, and concepts of a so-called "FAIR enabling organization" by this use of the name FAIR for institutions. To determine whether a research organization (research institute, university, or university of applied sciences) is FAIR, the task group produced two products: \textbf{(1)} a definition for a FAIR enabling organization and \textbf{(2)} a self-assessment tool. The self-assessment tool is a straightforward instrument that is available in editable PDF format. Users will be able to determine the true FAIR-ness by responding to the questions and determining the level (beginning, intermediate, or advanced) at which user evaluates the performance of their company. Furthermore, by utilizing the data at the more advanced level(s), the user may create a road map for their organization to become a FAIR Enabling Research Organization.

FAIRness self-assessment grids: In order to enhance research crediting and rewarding systems for scientists who want to organize their data (and material resources) for community sharing, the Research Data Alliance's SHARC Interest Group was founded. For this to happen, data must be easily found and accessed on the Internet, adhere to common standards, and be compatible with one another and the FAIR principles to be reused. It requires a significant investment of time, effort, knowledge, and commitment. Encouraging scientists to share their data requires streamlining the processes. To do this, data-sharing procedures must support FAIR principles compliance processes and augment human comprehension of FAIRness criteria, hence encouraging FAIRness literacy, rather than only focusing on the criterion's machine-readability. The initial criteria to be determined in the FAIRness evaluation procedures and roadmap must be appropriate and comprehensible to humans. This template may be used again and again to enhance the FAIRification evaluation process by providing human-readable standards. Prioritizing the most suitable and adequate activities, assistance, and training may be done based on the degree of compliance for each criterion.
"Templates for FAIRness evaluation criteria - RDA-SHARC ig" is the title of a paper that was published on June 29, 2020. Version 1.1 of the publication has an Excel file download option. This tool's key features are: 

     Comprehensive and speedy assessment grids
     Decision tree-like design
     Researcher-focused.




Other typed Tools ::::

Data Stewardship Wizard: The goal of Data Stewardship Wizard is to reduce the negative perception associated with data management planning by emphasizing the advantages of data management for both the researcher and the research project, rather than the responsibilities. Examples of this include identifying appropriate tools that can assist in compiling and maintaining provenance metadata or important information standards. The FAIR principles—which state that data should be Findable, Accessible, Interoperable, and Reusable for Both Machines and Humans—are clearly shown by the Data Stewardship Wizard as a result of each response. The primary objective of the Data Stewardship Wizard is to evaluate a whole project, rather than just one dataset. Throughout the project, this tool will require a great deal of input. It consists of open-ended, multiple-choice, and questions that require a GUID, much like a DOI. The Data Stewardship Wizard is intended to be utilized as an independent tool right from the outset of a project. This implies that some data can be added even before other data has been created. This tool is intended to facilitate the organization and development of a more FAIR and transparent data handling process. This tool is not suitable for users who wish to do a fast FAIR evaluation on an existing dataset. The Data Stewardship Wizard includes two extra principles being scored, namely the openness of the data and the data management. Additionally, the Data Stewardship Wizard provides advice on how to manage your data throughout the project. As a result, this tool is better suited for use as a guide for processing data during the project.

Tripple tool kit: GoTriple is a cutting-edge, multilingual platform for social science and humanities (SSH) discovery. It offers one of the primary access points for finding and repurposing research artifacts important to the wide range of disciplines that fall under the umbrella of SSH: GoTriple automatically imports publications and research data, project descriptions, and researcher profiles from aggregators and source providers, enriches them semantically, and links them. It consists of a collection of materials (a set of guidelines for planning online training events that are FAIR-by-design, template materials for outreach, communication, and outreach measurement, and survey instruments to evaluate training needs and impact) that are publicly accessible on Zenodo. 

FAIRplus: To enhance the state of FAIRness in research datasets, the FAIRplus-DSM model is meant to serve as an all-inclusive reference model. The DSM model identifies and categorizes requirements that contribute to a step-by-step improvement in the FAIRness level of a specific research dataset, based on the FAIR guiding principles. FAIR Cookbook" is closely linked to the FAIRplus initiative. Nonetheless, the goal of FAIRplus is to advance the concepts of FAIR data in the field of life sciences. To this end, it creates a range of resources, standards, and tools. The FAIR Cookbook's recipes are divided into audience categories to cater to everyone participating in the data management life cycle. They include advice, technical, hands-on, background, and review kinds that cover the operation phases of FAIR data management. If you work in the life sciences and need help putting the FAIR Principles into reality, this cookbook is for you.
FAIR DataSet Maturity (FAIR-DSM) Assessment Tool describes these steps for the assessment.
(Level 1) Basic objects with simple metadata enable basic discovery; a foundation for identifying FAIR data objects.
(Level 2) Improving structured data usability by enhancing dataset fields and values.
(Level 3) Data conforms to community standards, with advanced search and retrieval capabilities.
(Level 4) Focuses on cross-domain interoperability for larger integration projects.
(Level 5) Enterprise-level data management with comprehensive governance and reliability measures.

