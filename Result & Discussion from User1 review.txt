Result & Discussion ::::

The overall is described here together with the tool features, and the specifics of the tool assessment will be displayed in Table \ref{table 3}. The user's review and assessment-taking experience is depicted in the third table's columns. Additionally, this table offers suggestions on whether or not to assist another user with their study project. This advice is also predicated on an awareness of the output result and the tool's general usefulness and overall experience. Some tools provide a FAIR score to help users discover the parts of their data that are missing, while others provide advice on how to make their data more FAIR. As a result, the tools' usability and likeness improve.



An analysis from the perspective of User1 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>


---------------------------Online self-assessment survey-based tools ----------------------------------



FAIR Data Self-Assessment Tool:::::
Firstly, as User1 is already familiar with the fundamentals of the FAIR concept, discussing the FAIR tool, the ‘FAIR Data Self-Assessment Tool’, hosted by ARDC, takes User1, 10 minutes in total. The examination presented User1 with multiple-choice responses for every issue, and the outcome was a straightforward GREEN BAR for general concepts. However, there isn't a % bar shown, which makes things confusing but still acceptable. Since there are no qualitative or quantitative results displayed on the output page, this is the basic level of the evaluation. Thus, the user's experience was decent but not exceptional. Ultimately, the user will not suggest this tool to other users/researchers.
 

FAIR data Self-Assessment Tool:::::
The additional part of the first one is the ‘FAIR data Self-Assessment Tool’, which was developed by Cross-Lateral Enterprises Pty Ltd. Nonetheless, it has some intricate features. For example, every question will explain its meaning and guide users of all skill levels. As a result, answering each question is rather simple, and the evaluation will take 11 minutes and 22 seconds in total.  Each principle's green bar will be displayed along with a percentage in this evaluation. For Findable (88%), Accessible (80%), Interoperable (87%), Reusable (100%), and Overall (90%) User1 received percentage green bars. hence utilizing and understanding the tool is preferable. Further researchers have been encouraged to utilize this tool by User1.


FAIR-Aware:::::
'FAIR-Aware' is a tool that allows the user to examine their data's FAIR level and become aware of it. User1 spent 14 minutes and 44 seconds completing the evaluation, answering each question with a radio choice. It is clear that this tool will raise awareness of the FAIR principle because User1 received an outcome with Awareness (10/10) and readiness to comply (40/50). User1 had the best experience overall, and he or she will advise other researchers to make use of this tool and share what they learn.


FAIRdat:::::
In order to provide more information while responding to the questions, the tool ‘FAIRdat’ additionally combines radio and input textbox. Thus, the tool must select the response and, if necessary, add content. In 9 minutes and 22 seconds, User 1 completed the evaluation. Consequently, User 1 received five stars for being findable, five stars for being accessible, one star for being interoperable, and R=(F+A+I)/3 for being reusable. Reusability is not well thought out with this tool, and the output page does not display the outcome. Although User1 had a positive overall experience, User1 will not suggest the tool to other academics or users.


FairDataBR:::::
Another useful tool for taking the assessment is ‘FairDataBR’. In only 12 minutes and 34 seconds, User1 completed the evaluation by selecting the radio "yes" or "no" options and responding. This tool will yield an average pie chart as well as the outcome of a pie chart with a decimal score out of 10. User1 received scores of 9.40/10 for findability, 9.0/10 for accessibility, 10/10 for interoperability, 9.0/10 for reuse, and 9.35/10 on average. Even if this program has issues, the result is excellent in terms of graphics. Portuguese and English are the two languages for which this tool offers alternatives. As a result, the language is frequently altered while switching pages. Every choice selected for an answer is immediately erased when switching the language manually from Portuguese to English. Accordingly, User1 encountered problems initially while responding to them. The primary problem with the tool is this, thus, even if it has some positive aspects for the user experience, User1 will not suggest this to other users/researchers.


NFDI4Culture FAIR-Check:::::
The 'NFDI4Culture FAIR-Check' tool just has 25 multiple-choice questions that are easy to answer, but they take a lot of time and effort for a first-level user to complete. Because User 1 is aware of the FAIR principle, the task took 14 minutes and 17 seconds to do; if not, it would have taken longer than 30 minutes, which is not acceptable for any tool. Consequently, this tool has not produced a specific FAIR score; instead, it just downloads a PDF containing absurd responses. In my experience, this tool isn't the greatest, and recommending it to other researchers isn't suitable either.


SATIFYD:::::
The last alternative tool that features drop-down, multiple-choice, and multi-select questions is SATIFYD. The tool features beautiful graphics and a very straightforward design. User 1 completed the evaluation in 10 minutes and 59 seconds since there were fewer questions. The findings that User1 was able to find were Findable(88%), Accessible(100%), Interoperable(58%), Reusable(85%), and generally (83%). Beautiful graphics displaying the weight of the response value in blue within the letters F, A, I, and R are included in the output page. Upon completion of each section of the response, a percentage will be displayed with blue-colored filled letters, indicating a final score that is fair and appropriate. This tool is highly recommended by User1, who also reports having had the best experience with it.



----------------------------------Online semi-automated tools---------------------------------



FAIRshake:::::
As User1 had taken the assessment in this FAIRshake tool, at first, have to create a digital object and then select FAIR questions/metrics The next step is to take the FAIR assessment and then finally get the result of the FAIR insignia. Fair Insignia is a 3x3 colored rubric because there are a total of 9 questions. Based on the answers, each grid will be turned into red, blue, and grey colors. Blue represents 100\% positive answers, and red means 100% negative. (Blue-100%, purple-75%, wine red-25% and red-0%) grey means not answered.
Using the FAIRShake tool, User1 has completed two FAIR tests. The first, which User1 chose as a typical project, had the project's digital item listed as the research paper's title, with the addition of her named rubrics, which she made by adding metrics based on the F1, F2, F3, F4, A1, and A2 concepts. In the context of that, after completing the form, she was allowed to access it. After completing the self-assessment, she selected "yes," "yesbut," "no," and "nobut" from the alternatives that appeared on the screen depending on the metrics. Based on that, she received a colored rubric with her evaluation results after completing it. Red is 100\% negative, blue is 100\% positive, and the other two are light red and light blue, respectively, for yes-and-nobut options.
For the second test, she made a project called "SoBigData" and specified the repository type after the creation of the project's digital item for evaluation. She completed the form by providing the name of the digital object as the research paper's title, adding further details, and using the pre-existing rubric, "FAIRShake repository test," as a rubric. She then saw the "Access" option and began the self-assessment, answering the nine questions that the rubric's owner had previously included. The questions are the metrics that are already provided in that rubric. Thus, she received the colored rubric as a result of the test.
she spent at least two hours trying to fully understand this tool, which was pretty difficult to grasp at first. I'm still not sure if the self-evaluation is good enough to receive a FAIR result. To create their own, first-level users must be fully informed about all metrics. as the evaluation is conducted using a collection of questions that form up the metric. Therefore, what purpose does this tool have if the user is constructing their own metric!


----------------------------------Online automated tool---------------------------------------

FAIR enough:::::
User1 initially chooses the option in the FAIR enough tool website,  labeled "FAIR enough metadata maturity indicators (fair-enough-metadata)". The assessment took her one minute and seven seconds to complete. She received evaluation scores of 14 out of 16, and 87.5% of the log level was marked as "success, warnings, and failures." It performed poorly in the A2 measure "Persistent metadata" and the R1 metric "Metadata includes a standard license." as opposed to the 14 other indicators that User1 has in their metadata assessment. Following User 1's selection of the second choice, "FAIR Evaluator maturity indicator," the evaluation took 24 seconds to complete. It awarded an assessment score of 15 out of 22 and a percentage of 68.18%. It was unsuccessful in metric F1 is "FAIR metric Gen2 ( Data identifier persistence)", followed by F4 - "FAIR metric Gen2(Searchable in major search engine )", A 2 - "FAIR metric Gen2(Metadata persistence)", I1 to "FAIR metric Gen2(Data knowledge representation language for strong and weak)" both of them, I2 - "FAIR metric Gen2(Metadata uses FAIR vocabularies for strong)", R1.2 - "FAIR metric Gen2(Metadata includes license for strong)". In the final assessment, User 1 took 16 seconds to complete and received an evaluation score of 18 out of 22, or 81.82%, as a percentage. and saw "success, warnings, and failures" in the log level. It displays issues in the metrics below as a failure log level: F 1 "Data identifier is persistent", F 3 "Data identifier is explicitly in metadata", A 2 "Metadata is persistent", R 1 "Metadata includes a standard license" . At the bottom of the page, this tool offers the option to "Download the evaluation results JSON file" for each assessment.
   

F-UJI:::::
After completing the evaluation using this automated tool F-UJI, User1 took 1 minute and 30 seconds to run the tool, 7 minutes and 29 seconds to understand the tool's output, and 8.59 seconds overall. This tool has an excellent output design with three steps that are Evaluate resource, summary, and Report. In first part 'Evaluate resource' it has these points:
FAIR level:  Moderate
Resource PID/URL: \url{https://doi.org/10.13026/cerq-fc86}
DataCite support: enabled
Metric Version:	metrics\_v0.5
Metric Specification: \url{https://doi.org/10.5281/zenodo.6461229}
Software version: 3.1.0
Download assessment results: {JSON}
Save and share assessment results:
The tool provided a pie chart with the level for each principle and the FAIR percentage of 62% in the summary section. With a score acquired, User 1 received 'Advanced' in Findable, 'Moderate' in Accessible and Interoperable, and 'Initial' in Reusable. In terms of findability, User 1 received a score of 6 out of 7, accessibility, 3 out of 4, interoperability, and reusability, 2 out of 3. 
The third section of the tests included information metric names, test names, details of the test, score, maturity, debug message, and results shown as a colored checkmark. Thus, a green checkmark indicates an advanced result; a light green indicates a moderated outcome; a yellow tick indicates an initial result; and a grey question mark indicates an incomplete result. User1 found the output result easily understandable and will suggest this tool to others for future usage.


FAIR-Checker:::::
There are two options on this online tool FAIR-Checker page: inspect and check. Thus, User 1 selected the first option and used the URL link to check the condition of the FAIR resource. After entering the DOI, User 1 completed the examination in two tries. The first run took 27 minutes to complete; after stopping, the second run took 42 seconds. Thus, a network fault might have been the cause of this. The output, which includes an explanation for each measure, is likewise highly defined and straightforward. A list of metrics with descriptions and outcomes as well as a Radar chart of metrics completion are supplied on the output page. This radar chart will visually represent the FAIR percentage of each principle's fulfillment. For User 1, all principles are 100\% met, however only Interoperable is 66.7%.
The second section includes the following: Principles, their names, a description, comments on the state of the principles, as well as tests, results, scores, and other details. User 1 received recommendations for these metrics: F2A, I1, and I3. Every measure received a score of 2, although only the scores for F2A and I1 were 1 and I3 was 0. Therefore, only I3 was unsuccessful. Additionally, there is an option to export the result, which may be downloaded as a CSV file simply by clicking the button.
To improve the quality of the metadata, there is an additional option on the main page called Inspect. When User 1 entered the URL link on the 'Step 1: collect RDF metadata from the resource URL,' 'Enrich Graph' appeared as the Step 2 result. There are three alternatives for this enriched graph to choose from: "from Wikipedia," "from OpenCitation," and "from OpenAIRE." Three more options—JSON-LD and Trig—allow you to modify the outcome of the second step. Clicking these two buttons will allow the users to get the desired outcome for themselves. 'Metadata quality checks' is the third phase. As the option to see the result with classes and attributes was previously there, User 1 checked the vocabulary in this instance. The tool features a Knowledge Graph based on relations (properties) and ontology ideas (classes). Furthermore, these classes and attributes are already listed in reference ontology registries like BioPortal, OLS, and LOV.


FOOPS!:::::
With this tool FOOPS!, User 1 received a 15\% result for overall output throughout the test. Just URI was required by this tool to complete the test, and the whole process took 17.35 seconds. It also displays every FAIR check along with measurements and dimensions for every percentile segment in less time. The entire procedure explains how to produce the solution simply and is easy to grasp. USER 1 did not have a positive experience using this tool, as she received a score of 1 out of 9, 1 out of 9, 1.50 out of 3, 0 out of 3, and Reusable 1 out of 9. Before using the tool, users can read published articles from the program's creator, Universidad Politécnica de Madrid. Therefore, several features are still lacking from this tool, preventing User 1 from recommending it to others for employment in the future. The organization itself highlighted some future work in the released study. To improve the ontology documentation, the generated report can be downloaded, more externally supported registries can be added, and their current recommendations—which include automatically suggesting prefixes in well-known prefix registries—will be improved with actionable guidelines.


OpenAIRE Validator - FAIR assessment:::::
After using the tool OpenAIRE Validator - FAIR assessment for 10 minutes and 41 seconds, User 1 was unable to complete the test because the program displayed error messages and did not function. The error message is “The base URL could not be confirmed. Make sure that you are using the correct protocol ("http" or "https"), as the service is currently unable to follow URL redirects. If the warning persists, leave us a note in the comments section. We will check your interface and contact you if there are any problems.”To utilize this tool, User 1 first registered on the OpenAIRE website and then activated her account using the activation code she received in her mail. Subsequently, she proceeded to access the website and selected the PROVIDE option to undertake the FAIR test assessment. She had to quit during the first stage when she was supposed to give the URL of the dataset since this online tool was displaying an error warning.


FAIR EVA (Evaluator, Validator & Advisor):::::
When User1 first attempted to use this tool FAIR EVA (Evaluator, Validator & Advisor), she attempted to click on the link provided in the research article about it. Although the website is only in Spanish, which makes evaluation difficult, online translation is available, thus User 1 first gave the PID as requested by the website. However, an error message stating, "It seems there has been a problem”.  All this process took 3 mins 2 secs. For local hosting, then User1 began to study how to use a standalone or docker platform to run the code. Even though the Readme.txt file included the GitHub code and instructions, it took a while to figure out how to run the code. Installing the libraries and modules, as well as the software required to run this code, took 73 minutes and 53 seconds. Additionally, it did not solve the "500 Internal Server Error" internal server problem. As a result, User1 is unable to evaluate this tool and is unable to run the program autonomously or visit the website.


HowFAIRIs:::::
User1 installs this software tool named HowFAIRIs onto her local system using the Python Command Prompt and uses it as software. User1 completed the 'livetest' using data that was already given by the organization; they just verified the data's fairness and do not possess a repository. Following the tool's installation, User 1 took ten minutes and ten seconds to complete the test, which provided a good result with good compliance (a green color and five out of five recommendations). This tool returned the following assertionError when used with an online DOI URL: ‘Repository should be on github.com or on gitlab.com’.



----------------------------------------Offline self-assessment survey based-----------------------------------------------------



Do I-PASS for FAIR:::::
The LCRDM Task Group "FAIR Enabling Research Organization" created this self-assessment tool Do I-PASS for FAIR, which was published on November 3, 2020. In a published article, User1 has the opportunity to download the editable PDF and complete the evaluation by providing the answers to the questions and rating the level (beginning, intermediate, or advanced). To complete the evaluation for the dataset's performance, it took 22 minutes and 3 seconds. An organization uses this tool to identify whether true fairness is. Since User 1 is not the organization's administrator and is merely assessing the FAIRNESS of a certain dataset, several of her questions seemed pointless to her. There are 22 questions on the tool, 19 of which are multiple choice and 3 of which are open-ended. There are three options available for these multiple-choice questions: "Beginner," "Intermediate," and "Advanced." The 19 questions in this tool are separated into 5 sections. Four questions are in the "Policy" part, five in the "Services" section, three in the "Skills" section, two in the "Incentives" section, and five in the "Adoption" section. An organization that conducts research and employs a committed team of data professionals is known as FAIR Enabling Research Organization (FERO). Its policies are focused on providing technical, organizational, and infrastructural facilities and services that facilitate the creation and publication of FAIR data by researchers. These questions provide the institutes with a score based on FAIRness. This self-editable PDF can be shared by an organization with other employees so they can use the material at a more advanced level to maintain the roadmap for becoming a FAIR Enabling Research Organization(s).



FAIRness self-assessment grids:::::
For User1, this tool FAIRness self-assessment grids is a great experience even if it requires a solid understanding of the FAIR concept. Anybody can utilize this online self-assessment tool, which is accessible via the website. There are two choices for the assessment: the extensive template and the minimal template. This is a six-page Excel sheet document. After completing the evaluation for the extensive template, User1 took 49 minutes and 46 seconds to comprehend the questions and provide an answer. Every principle contains a different number of questions, and the questions are based on a decision tree. There are 45 questions altogether, of which the answers to Findable 12, Accessible 11, Interoperable 5, and Reusable 17 will yield the overall score for FAIRness in four distinct alternatives ‘Never’, ‘Mandatory’, ‘Sometimes’, and ‘Always’. 
A set of criteria, a ‘Synthetic criteria tree’, a ‘glossary’, and ‘instructions’ are also included in this program. Therefore, the user can study the sheets to learn before using the tool if it is not comprehensible. When responding to the input with a "1," User1 finds it a bit difficult to grasp the questions and complete the grid. Once the evaluation is complete, the total score will also be displayed. The score will be calculated when ‘1’ is entered into the Excel grids. If the user is unfamiliar with the FAIR principle, responding to these decision tree questions will take a long time. Following completion of the Excel sheet's questions, the scores for each principle will be displayed out of a range of fixed marks. The Excel sheet has a beautiful overall representation, although it might be challenging to use at first. This tool is useful for sharing information since it includes a "Motivations for Sharing" part with several open-ended questions based on three criteria: "Essential," "Recommended," and "Desired." This section's goal is to raise awareness about the designated dataset for which the evaluation must be completed as well as the gaps in the FAIR principle. This sheet is an excellent choice for maintaining FAIRness for additional research because it can be shared with other users. For User1, the whole experience has been both excellent and challenging. And User 1 will advise using this tool even more to raise awareness.



----------------------------------------------Other type tools------------------------------------------------------------------------




Data Stewardship Wizard:::::
As FAIR Metrics in  Data Stewardship Wizard, Users may assess their performance in terms of FAIR metrics by analyzing their responses to each questionnaire. To make their study fairer, users may thus go back and review their choices as early as the planning stage. From a technological standpoint, users can install the application on the local system using Docker or without it. Using Docker, User 1 attempted to install the application locally on her Windows PC. Installing requirements, reading the process from the attached document, taking the test, performing according to directions, learning the platform, and other tasks took 49 minutes. While User1 downloads the tool locally, the only users present are the default ones, and no knowledge models are available at this time. It is highly challenging to handle this tool for personal research data because it is managed by academic institutions. Thus, User1 did not attempt to operate on this platform. Using the responses to the questions, this tool differs from the others examined in this study in that it attempts to forecast the future FAIR state of the data if the DMP (Data Management Plan) is implemented. Furthermore, responding to the questions indicates which response yields the greatest metrics scores. The tool provides a great deal of explanation for each question, making it useful for a diverse variety of users. Nevertheless, the tool's parts and questions are lengthy and intricate, taking a lot of time to finish. The result is a score between 0 and 1, where 1 is the most ideal, for openness, DMP practices, and FAIR. An additional feature that indicates which principle is falling behind is a radar graphic.


GoTripple:::::
This tool GoTripple functions as a repository by gathering various extracted metadata from projects. Personalized suggestions, sophisticated interactive visualizations, and more are features of this application. Based on language, this tool may automatically designate a discipline for a certain kind of document within the system. Based on the study topics, this program may automatically generate vocabulary in twelve different languages, making the paper easier for users to grasp. The GoTriple platform has a two-level architecture: 1. the Core pipeline - this is where the data imported from aggregators is ingested, processed, and enriched, and 2. the Discovery Platform - this is what end users see when accessing the website gotriple.eu and where the imported data is made available (for searching, filtering, annotating, downloading, etc.). GoTriple improves access to open content and resources and facilitates collaborations across disciplinary and language boundaries. Data sharing and usage according to the FAIR principles – Findable, Accessible, Interoperable, Reusable – is fostered.


FAIRplus:::::
The FAIR DataSet Maturity (FAIR-DSM) Assessment Tool in the big European project named FAIRplus has 17 questions that describe how to do the assessment. These questions include 14 indications for level 1, 21 indicators for level 2, 22 indicators for level 3, 14 indicators for level 4, and 5 indicators for level 5. It took 15 minutes and 22 seconds to complete the test, with medium comprehension checklist selections. Guidelines and materials to make the data FAIR are included in this FAIR COOKBOOK tool. The output page's description, which includes all maturity indicators, is simple for User 1 to figure out. 
